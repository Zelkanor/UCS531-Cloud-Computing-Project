{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "423ff01c-485d-4a09-bf75-1676822ade8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "\n",
    "# --- 1. CONFIGURATION ---\n",
    "# This is the API Gateway endpoint \n",
    "API_ENDPOINT = \"https://s35t79fgcl.execute-api.ap-south-1.amazonaws.com/dev/upload-url\"\n",
    "# ---\n",
    "\n",
    "def get_upload_url(filename):\n",
    "    \"\"\"\n",
    "    Calls Lambda API to get a presigned S3 URL.\n",
    "    \"\"\"\n",
    "    print(f\"1. Requesting upload URL for: {filename}\")\n",
    "    \n",
    "    try:\n",
    "        # We send the filename in the body, as the Lambda code expects.\n",
    "        response = requests.post(API_ENDPOINT, json={'filename': filename})\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            print(f\"   ‚ùå Error: Failed to get URL (Status {response.status_code})\")\n",
    "            print(f\"   Response: {response.text}\")\n",
    "            return None, None\n",
    "\n",
    "        # API Gateway proxy integration returns a JSON string in the 'body' key\n",
    "        try:\n",
    "            data = json.loads(response.json().get('body', response.text))\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"   ‚ùå Error: Could not decode JSON response: {response.text}\")\n",
    "            return None, None\n",
    "            \n",
    "        upload_url = data.get('upload_url')\n",
    "        doc_id = data.get('document_id')\n",
    "        \n",
    "        if not upload_url or not doc_id:\n",
    "            print(f\"   ‚ùå Error: Incomplete response from Lambda.\")\n",
    "            print(f\"   Lambda Response: {data}\")\n",
    "            return None, None\n",
    "\n",
    "        print(f\"   ‚úÖ Success. Document ID: {doc_id}\")\n",
    "        return upload_url, doc_id\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"   ‚ùå NETWORK ERROR: Could not connect to your API Gateway.\")\n",
    "        print(f\"   Check that the API_ENDPOINT is correct and deployed.\")\n",
    "        print(f\"   Error: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def upload_file_to_s3(upload_url, file_path):\n",
    "    \"\"\"\n",
    "    Uploads the actual file to the presigned URL.\n",
    "    (Renamed from 'upload_file' to avoid name conflict)\n",
    "    \"\"\"\n",
    "    print(f\"\\n2. Uploading {os.path.basename(file_path)} to S3...\")\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            file_data = f.read()\n",
    "        \n",
    "        # Make the PUT request with the raw file data\n",
    "        response = requests.put(upload_url, data=file_data)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            print(f\"   ‚úÖ SUCCESS! File upload complete.\")\n",
    "            return True\n",
    "        else:\n",
    "            # This will show the <Error> XML from S3 if it fails\n",
    "            print(f\"   ‚ùå UPLOAD FAILED (Status {response.status_code})\")\n",
    "            print(f\"   Response from S3: {response.text}\")\n",
    "            return False\n",
    "            \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"   ‚ùå NETWORK ERROR during upload. This could be a firewall.\")\n",
    "        print(f\"   Error: {e}\")\n",
    "        return False\n",
    "    except FileNotFoundError:\n",
    "        print(f\"   ‚ùå Error: Local file not found at {file_path}\")\n",
    "        return False\n",
    "\n",
    "def upload_document(file_path):\n",
    "    \"\"\"\n",
    "    Main function to run the complete upload process from a Jupyter cell.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): The local path to the file you want to upload.\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- Check Config ---\n",
    "    if \"YOUR-API-ID\" in API_ENDPOINT:\n",
    "        print(\"=\"*50)\n",
    "        print(\"‚ùå ERROR: Please edit the script (line 7)\")\n",
    "        print(\"   You must set your `API_ENDPOINT` variable.\")\n",
    "        print(\"=\"*50)\n",
    "        return\n",
    "\n",
    "    # --- Check File Path ---\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"‚ùå Error: File not found at {file_path}\")\n",
    "        return\n",
    "        \n",
    "    filename = os.path.basename(file_path)\n",
    "    \n",
    "    # --- Run Process ---\n",
    "    print(\"=\"*50)\n",
    "    upload_url, doc_id = get_upload_url(filename)\n",
    "    \n",
    "    if upload_url and doc_id:\n",
    "        success = upload_file_to_s3(upload_url, file_path)\n",
    "        if success:\n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "            print(\"üéâ Process Complete!\")\n",
    "            print(f\"   Document ID: {doc_id}\")\n",
    "            print(f\"   File: {filename}\")\n",
    "            print(\"\\n   The 'process-document' Lambda should now be triggered.\")\n",
    "            print(\"=\"*50)\n",
    "            return doc_id # Return the doc_id for use in other cells\n",
    "        else:\n",
    "            print(\"\\nUpload failed. Please check errors above.\")\n",
    "    else:\n",
    "        print(\"\\nCould not get upload URL. Aborting.\")\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bf619c9-773c-4ec3-b08f-d1beec9d8409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting upload for: pdfs/Attention Is All You Need.pdf ---\n",
      "==================================================\n",
      "1. Requesting upload URL for: Attention Is All You Need.pdf\n",
      "   ‚úÖ Success. Document ID: bad18322\n",
      "\n",
      "2. Uploading Attention Is All You Need.pdf to S3...\n",
      "   ‚úÖ SUCCESS! File upload complete.\n",
      "\n",
      "==================================================\n",
      "üéâ Process Complete!\n",
      "   Document ID: bad18322\n",
      "   File: Attention Is All You Need.pdf\n",
      "\n",
      "   The 'process-document' Lambda should now be triggered.\n",
      "==================================================\n",
      "------------------------------------\n",
      "\n",
      "--- Starting upload for: pdfs/Deep Residual Learning for Image Recognition.pdf ---\n",
      "==================================================\n",
      "1. Requesting upload URL for: Deep Residual Learning for Image Recognition.pdf\n",
      "   ‚úÖ Success. Document ID: c3906487\n",
      "\n",
      "2. Uploading Deep Residual Learning for Image Recognition.pdf to S3...\n",
      "   ‚úÖ SUCCESS! File upload complete.\n",
      "\n",
      "==================================================\n",
      "üéâ Process Complete!\n",
      "   Document ID: c3906487\n",
      "   File: Deep Residual Learning for Image Recognition.pdf\n",
      "\n",
      "   The 'process-document' Lambda should now be triggered.\n",
      "==================================================\n",
      "------------------------------------\n",
      "\n",
      "--- Starting upload for: pdfs/Generative Adversarial Networks.pdf ---\n",
      "==================================================\n",
      "1. Requesting upload URL for: Generative Adversarial Networks.pdf\n",
      "   ‚úÖ Success. Document ID: 342a7a1b\n",
      "\n",
      "2. Uploading Generative Adversarial Networks.pdf to S3...\n",
      "   ‚úÖ SUCCESS! File upload complete.\n",
      "\n",
      "==================================================\n",
      "üéâ Process Complete!\n",
      "   Document ID: 342a7a1b\n",
      "   File: Generative Adversarial Networks.pdf\n",
      "\n",
      "   The 'process-document' Lambda should now be triggered.\n",
      "==================================================\n",
      "------------------------------------\n",
      "\n",
      "--- Starting upload for: pdfs/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf ---\n",
      "==================================================\n",
      "1. Requesting upload URL for: BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf\n",
      "   ‚úÖ Success. Document ID: be6d073f\n",
      "\n",
      "2. Uploading BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf to S3...\n",
      "   ‚úÖ SUCCESS! File upload complete.\n",
      "\n",
      "==================================================\n",
      "üéâ Process Complete!\n",
      "   Document ID: be6d073f\n",
      "   File: BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf\n",
      "\n",
      "   The 'process-document' Lambda should now be triggered.\n",
      "==================================================\n",
      "------------------------------------\n",
      "\n",
      "--- Starting upload for: pdfs/A Survey of Large Language Models.pdf ---\n",
      "==================================================\n",
      "1. Requesting upload URL for: A Survey of Large Language Models.pdf\n",
      "   ‚úÖ Success. Document ID: ac5ca20f\n",
      "\n",
      "2. Uploading A Survey of Large Language Models.pdf to S3...\n",
      "   ‚úÖ SUCCESS! File upload complete.\n",
      "\n",
      "==================================================\n",
      "üéâ Process Complete!\n",
      "   Document ID: ac5ca20f\n",
      "   File: A Survey of Large Language Models.pdf\n",
      "\n",
      "   The 'process-document' Lambda should now be triggered.\n",
      "==================================================\n",
      "------------------------------------\n",
      "\n",
      "\n",
      "--- All Uploads Complete! ---\n",
      "Uploaded Document IDs:\n",
      "{\n",
      "  \"pdfs/Attention Is All You Need.pdf\": \"bad18322\",\n",
      "  \"pdfs/Deep Residual Learning for Image Recognition.pdf\": \"c3906487\",\n",
      "  \"pdfs/Generative Adversarial Networks.pdf\": \"342a7a1b\",\n",
      "  \"pdfs/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf\": \"be6d073f\",\n",
      "  \"pdfs/A Survey of Large Language Models.pdf\": \"ac5ca20f\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# --- RUN BATCH UPLOAD ---\n",
    "\n",
    "files_to_upload = [\n",
    "    \"pdfs/Attention Is All You Need.pdf\", \n",
    "    \"pdfs/Deep Residual Learning for Image Recognition.pdf\", \n",
    "    \"pdfs/Generative Adversarial Networks.pdf\",  \n",
    "    \"pdfs/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf\", \n",
    "    \"pdfs/A Survey of Large Language Models.pdf\"  \n",
    "]\n",
    "\n",
    "document_ids = {} # To store all the new IDs\n",
    "\n",
    "for file in files_to_upload:\n",
    "    print(f\"--- Starting upload for: {file} ---\")\n",
    "\n",
    "    doc_id = upload_document(file) \n",
    "    \n",
    "    if doc_id:\n",
    "        document_ids[file] = doc_id\n",
    "    print(\"------------------------------------\\n\")\n",
    "\n",
    "print(\"\\n--- All Uploads Complete! ---\")\n",
    "print(\"Uploaded Document IDs:\")\n",
    "print(json.dumps(document_ids, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "867cbc62-3b95-479e-b3bf-3155cbb6f3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "# --- 1. CONFIGURATION ---\n",
    "# API GATEWAY INVOKE URL \n",
    "API_BASE_URL = \"https://s35t79fgcl.execute-api.ap-south-1.amazonaws.com\"\n",
    "# ---\n",
    "\n",
    "QUERY_ENDPOINT = f\"{API_BASE_URL}/dev/query\"\n",
    "ANSWER_ENDPOINT = f\"{API_BASE_URL}/dev/answer\"\n",
    "\n",
    "\n",
    "def test_full_rag_pipeline(question, doc_id=None):\n",
    "    \"\"\"\n",
    "    Calls /query to get chunks, then /answer to get a final response.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"=\"*60)\n",
    "    print(f\"RAG Pipeline Test\")\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Document ID: {doc_id or 'All Documents'}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # --- STEP 1: RETRIEVAL (/query) ---\n",
    "    print(\"\\n1. Calling /query endpoint to find relevant chunks...\")\n",
    "    query_payload = {\n",
    "        \"question\": question,\n",
    "        \"document_id\": doc_id,\n",
    "        \"top_k\": 5  # Request top 5 chunks for better context\n",
    "    }\n",
    "\n",
    "    response_query = None  # Initialize for error logging\n",
    "\n",
    "    try:\n",
    "        response_query = requests.post(QUERY_ENDPOINT, json=query_payload)\n",
    "\n",
    "        if response_query.status_code != 200:\n",
    "            print(f\"‚ùå /query FAILED (Status {response_query.status_code})\")\n",
    "            print(f\"   Response: {response_query.text}\")\n",
    "            return\n",
    "\n",
    "        response_json = response_query.json()\n",
    "        if \"body\" in response_json:\n",
    "            print(\"   (Proxy response detected)\")\n",
    "            query_data = json.loads(response_json[\"body\"])\n",
    "        else:\n",
    "            print(\"   (Non-proxy response detected)\")\n",
    "            query_data = response_json\n",
    "       \n",
    "        top_chunks = query_data.get('top_chunks', [])\n",
    "\n",
    "        if not top_chunks:\n",
    "            print(\"‚ùå /query SUCCEEDED, but no relevant chunks were found.\")\n",
    "            return\n",
    "\n",
    "        print(f\"‚úÖ /query Success. Found {len(top_chunks)} relevant chunks.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during /query step: {e}\")\n",
    "        if response_query:\n",
    "            print(f\"   Raw response text that caused error: {response_query.text}\")\n",
    "        return\n",
    "\n",
    "    # --- STEP 2: AUGMENTED GENERATION (/answer) ---\n",
    "    print(\"\\n2. Calling /answer endpoint to generate a final answer...\")\n",
    "    answer_payload = {\n",
    "        \"question\": question,\n",
    "        \"top_chunks\": top_chunks  # Pass the chunks found\n",
    "    }\n",
    "\n",
    "    response_answer = None  # Initialize for error logging\n",
    "\n",
    "    try:\n",
    "        response_answer = requests.post(ANSWER_ENDPOINT, json=answer_payload)\n",
    "\n",
    "        if response_answer.status_code != 200:\n",
    "            print(f\"‚ùå /answer FAILED (Status {response_answer.status_code})\")\n",
    "            print(f\"   Response: {response_answer.text}\")\n",
    "            return\n",
    "\n",
    "        # Parse proxy / non-proxy formats\n",
    "        response_json_answer = response_answer.json()\n",
    "        if \"body\" in response_json_answer:\n",
    "            print(\"   (Proxy response detected)\")\n",
    "            answer_data = json.loads(response_json_answer[\"body\"])\n",
    "        else:\n",
    "            print(\"   (Non-proxy response detected, parsing directly)\")\n",
    "            answer_data = response_json_answer\n",
    "            \n",
    "        final_answer = answer_data.get('answer', 'No answer found.')\n",
    "        sources = answer_data.get('sources', [])\n",
    "\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"üéâ RAG PIPELINE SUCCEEDED!\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "        print(\"\\nFINAL ANSWER:\\n\")\n",
    "        print(final_answer)\n",
    "\n",
    "        print(\"\\n\\nSOURCES USED:\\n\")\n",
    "        for i, source in enumerate(sources, 1):\n",
    "            print(f\"  {i}. {source['filename']} (Index: {source['chunk_index']}, Score: {source['similarity_score']})\")\n",
    "            print(f\"     Excerpt: \\\"{source['excerpt']}\\\"\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during /answer step: {e}\")\n",
    "        if response_answer:\n",
    "            print(f\"   Raw response text that caused error: {response_answer.text}\")\n",
    "        return\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6085c148-766a-4379-9084-a61ea20554d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "RAG Pipeline Test\n",
      "Question: What is the methodology of this paper?\n",
      "Document ID: bad18322\n",
      "============================================================\n",
      "\n",
      "1. Calling /query endpoint to find relevant chunks...\n",
      "   (Non-proxy response detected)\n",
      "‚úÖ /query Success. Found 5 relevant chunks.\n",
      "\n",
      "2. Calling /answer endpoint to generate a final answer...\n",
      "   (Non-proxy response detected, parsing directly)\n",
      "\n",
      "============================================================\n",
      "üéâ RAG PIPELINE SUCCEEDED!\n",
      "============================================================\n",
      "\n",
      "FINAL ANSWER:\n",
      "\n",
      "The paper describes a model that uses an encoder to map an input sequence to continuous representations and an auto-regressive decoder that generates an output sequence one element at a time, consuming previously generated symbols as input. It also details experiments involving parameter selection (e.g., dropout, learning rates, beam size) on a development set, and during inference, it uses beam search with a beam size of 4 and length penalty, averaging the last 20 checkpoints.\n",
      "\n",
      "\n",
      "SOURCES USED:\n",
      "\n",
      "  1. Attention Is All You Need.pdf (Index: 61, Score: 0.1573)\n",
      "     Excerpt: \"tokens for the semi-supervised setting. We performed only a small number of experiments to select the dropout, both attention and residual (section 5....\"\n",
      "\n",
      "  2. Attention Is All You Need.pdf (Index: 67, Score: 0.1513)\n",
      "     Excerpt: \"com/ tensorflow/tensor2tensor. Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments, corrections and ins...\"\n",
      "\n",
      "  3. Attention Is All You Need.pdf (Index: 15, Score: 0.1415)\n",
      "     Excerpt: \"an input sequence of symbol representations (x‚ÇÅ,...,x‚Çô) to a sequence of continuous representations Z = (z‚ÇÅ,...,z‚Çô). Given z, the decoder then generat...\"\n",
      "\n",
      "  4. Attention Is All You Need.pdf (Index: 74, Score: 0.1398)\n",
      "     Excerpt: \"nternational Conference on Learning Representations, 2017. [20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 201...\"\n",
      "\n",
      "  5. Attention Is All You Need.pdf (Index: 53, Score: 0.1283)\n",
      "     Excerpt: \"eraged the last 20 checkpoints. We used beam search with a beam size of 4 and length penalty Œ± = 0.6 [38]. These hyperparameters were chosen after exp...\"\n",
      "\n",
      "\n",
      "############################################################\n",
      "\n",
      "============================================================\n",
      "RAG Pipeline Test\n",
      "Question: What are the main conclusions?\n",
      "Document ID: bad18322\n",
      "============================================================\n",
      "\n",
      "1. Calling /query endpoint to find relevant chunks...\n",
      "   (Non-proxy response detected)\n",
      "‚úÖ /query Success. Found 5 relevant chunks.\n",
      "\n",
      "2. Calling /answer endpoint to generate a final answer...\n",
      "   (Non-proxy response detected, parsing directly)\n",
      "\n",
      "============================================================\n",
      "üéâ RAG PIPELINE SUCCEEDED!\n",
      "============================================================\n",
      "\n",
      "FINAL ANSWER:\n",
      "\n",
      "The main conclusions are that the Transformer model outperforms previous models on the WMT 2014 English-to-German translation task, and label smoothing improves accuracy and BLEU score despite hurting perplexity. The authors also plan to extend the Transformer to other modalities and tasks, and investigate efficient attention mechanisms for large inputs like images, audio, and video.\n",
      "\n",
      "\n",
      "SOURCES USED:\n",
      "\n",
      "  1. Attention Is All You Need.pdf (Index: 50, Score: 0.1365)\n",
      "     Excerpt: \"nd the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of Pd·µ£‚Çí‚Çö = 0.1. Label Smoothing During training,...\"\n",
      "\n",
      "  2. Attention Is All You Need.pdf (Index: 66, Score: 0.1343)\n",
      "     Excerpt: \"attention-based models and plan to apply them to other tasks. We plan to extend the Transformer to problems involving input and output modalities othe...\"\n",
      "\n",
      "  3. Attention Is All You Need.pdf (Index: 53, Score: 0.1291)\n",
      "     Excerpt: \"eraged the last 20 checkpoints. We used beam search with a beam size of 4 and length penalty Œ± = 0.6 [38]. These hyperparameters were chosen after exp...\"\n",
      "\n",
      "  4. Attention Is All You Need.pdf (Index: 4, Score: 0.1288)\n",
      "     Excerpt: \"Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed ...\"\n",
      "\n",
      "  5. Attention Is All You Need.pdf (Index: 62, Score: 0.1246)\n",
      "     Excerpt: \"raining WSJ 23 F1 Vinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3 Petrov et al. (2006) [29] WSJ only, discriminative 90.4 Zhu et al....\"\n",
      "\n",
      "\n",
      "############################################################\n",
      "\n",
      "============================================================\n",
      "RAG Pipeline Test\n",
      "Question: Summarize the project this paper is about.\n",
      "Document ID: bad18322\n",
      "============================================================\n",
      "\n",
      "1. Calling /query endpoint to find relevant chunks...\n",
      "   (Non-proxy response detected)\n",
      "‚úÖ /query Success. Found 5 relevant chunks.\n",
      "\n",
      "2. Calling /answer endpoint to generate a final answer...\n",
      "   (Non-proxy response detected, parsing directly)\n",
      "\n",
      "============================================================\n",
      "üéâ RAG PIPELINE SUCCEEDED!\n",
      "============================================================\n",
      "\n",
      "FINAL ANSWER:\n",
      "\n",
      "The provided excerpts do not contain information to summarize the project the paper is about.\n",
      "\n",
      "\n",
      "SOURCES USED:\n",
      "\n",
      "  1. Attention Is All You Need.pdf (Index: 77, Score: 0.2158)\n",
      "     Excerpt: \"cessing, 2016. [28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive summarization. arXiv preprint arXiv: 170...\"\n",
      "\n",
      "  2. Attention Is All You Need.pdf (Index: 53, Score: 0.2048)\n",
      "     Excerpt: \"eraged the last 20 checkpoints. We used beam search with a beam size of 4 and length penalty Œ± = 0.6 [38]. These hyperparameters were chosen after exp...\"\n",
      "\n",
      "  3. Attention Is All You Need.pdf (Index: 78, Score: 0.2046)\n",
      "     Excerpt: \"ress and Lior Wolf. Using the output embedding to improve language models. arXiv preprint 1608.05859, 2016. [31] Rico Sennrich, Barry Haddow, and Alex...\"\n",
      "\n",
      "  4. Attention Is All You Need.pdf (Index: 5, Score: 0.1877)\n",
      "     Excerpt: \"emented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, w...\"\n",
      "\n",
      "  5. Attention Is All You Need.pdf (Index: 63, Score: 0.1797)\n",
      "     Excerpt: \"Kaiser el al. (2014) [37] semi-supervised 92.1 Transformer (4 layers) semi-supervised 92.7 Luong et al. (2015) [23] multi-task 93.0 Dyer et al. (2016)...\"\n",
      "\n",
      "\n",
      "############################################################\n",
      "\n",
      "============================================================\n",
      "RAG Pipeline Test\n",
      "Question: What is a Use Case Diagram according to this document?\n",
      "Document ID: bad18322\n",
      "============================================================\n",
      "\n",
      "1. Calling /query endpoint to find relevant chunks...\n",
      "   (Non-proxy response detected)\n",
      "‚úÖ /query Success. Found 5 relevant chunks.\n",
      "\n",
      "2. Calling /answer endpoint to generate a final answer...\n",
      "   (Non-proxy response detected, parsing directly)\n",
      "\n",
      "============================================================\n",
      "üéâ RAG PIPELINE SUCCEEDED!\n",
      "============================================================\n",
      "\n",
      "FINAL ANSWER:\n",
      "\n",
      "The provided excerpts do not contain any information about a \"Use Case Diagram.\"\n",
      "\n",
      "\n",
      "SOURCES USED:\n",
      "\n",
      "  1. Attention Is All You Need.pdf (Index: 60, Score: 0.0913)\n",
      "     Excerpt: \"e to attain state-of-the-art results in small-data regimes [37]. We trained a 4-layer transformer with d‚Çò‚Çídel = 1024 on the Wall Street Journal (WSJ) ...\"\n",
      "\n",
      "  2. Attention Is All You Need.pdf (Index: 44, Score: 0.0906)\n",
      "     Excerpt: \"xamples in the appendix. Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to t...\"\n",
      "\n",
      "  3. Attention Is All You Need.pdf (Index: 87, Score: 0.0808)\n",
      "     Excerpt: \"lication be perfect but its should be just this missing is what we are my opinion <EOS> <pad> - I - in - , , . The Law will never be perfect but its s...\"\n",
      "\n",
      "  4. Attention Is All You Need.pdf (Index: 0, Score: 0.076)\n",
      "     Excerpt: \"Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalis...\"\n",
      "\n",
      "  5. Attention Is All You Need.pdf (Index: 57, Score: 0.075)\n",
      "     Excerpt: \"3 300K 4.33 26.4 213 development set, newstest2013. We used beam search as described in the previous section, but no checkpoint averaging. We present ...\"\n",
      "\n",
      "\n",
      "############################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MY_DOCUMENT_ID = \"bad18322\"\n",
    "\n",
    "questions_to_ask = [\n",
    "    \"What is the methodology of this paper?\",\n",
    "    \"What are the main conclusions?\",\n",
    "    \"Summarize the project this paper is about.\",\n",
    "    \"What is a Use Case Diagram according to this document?\"\n",
    "]\n",
    "\n",
    "\n",
    "for q in questions_to_ask:\n",
    "    test_full_rag_pipeline(q, MY_DOCUMENT_ID)\n",
    "    print(\"\\n\" + \"###\"*20 + \"\\n\")  \n",
    "    time.sleep(1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284fcc75-d109-447c-b49f-5087c0eaef38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearn",
   "language": "python",
   "name": "deeplearn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
