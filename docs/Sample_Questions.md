Sample Questions for RAG Pipeline
=================================

Here is a list of sample questions, categorized by the research paper you've ingested. You can use these to test your test\_full\_rag\_pipeline function.

### 1\. "Attention Is All You Need" (The Transformer)

_(Use the document\_id you got from uploading 1706.03762.pdf)_

*   "What is the main idea of this paper?"
    
*   "What is a 'Transformer' according to this document?"
    
*   "What model architecture does the Transformer use?"
    
*   "What is 'multi-head attention'?"
    
*   "How does the Transformer's architecture differ from recurrent neural networks (RNNs)?"
    
*   "What were the main results on the WMT 2014 English-to-German translation task?"
    
*   "Summarize the section on 'Scaled Dot-Product Attention'."
    

### 2\. "Deep Residual Learning..." (ResNet)

_(Use the document\_id you got from uploading 1512.03385.pdf)_

*   "What is the 'degradation' problem that this paper addresses?"
    
*   "What is a 'residual block' or 'shortcut connection'?"
    
*   "What is the core architectural innovation of ResNet?"
    
*   "How did ResNet perform on the ImageNet 2015 competition?"
    
*   "What is the difference between ResNet-34 and ResNet-50?"
    

### 3\. "Generative Adversarial Nets" (GANs)

_(Use the document\_id you got from uploading 1406.2661.pdf)_

*   "What are the two main components of a Generative Adversarial Net?"
    
*   "Describe the objective function for a GAN."
    
*   "What is the role of the 'generator' in a GAN?"
    
*   "What is the 'discriminator's' job?"
    
*   "What were the main results on the MNIST and TFD datasets?"
    

### 4\. "BERT: Pre-training..." (BERT)

_(Use the document\_id you got from uploading 1810.04805.pdf)_

*   "What does 'BERT' stand for?"
    
*   "What are the two pre-training tasks BERT uses?"
    
*   "What is the 'Masked LM' (MLM) task?"
    
*   "How is BERT different from models like OpenAI's GPT that are uni-directional?"
    
*   "What is 'Next Sentence Prediction' (NSP)?"
    
*   "Summarize the SQuAD v1.1 results."
    

### 5\. "A Survey of Large Language Models"

_(Use the document\_id you got from uploading 2303.18223.pdf)_

*   "What are the four main pre-training objectives for LLMs mentioned in this survey?"
    
*   "Summarize the 'scaling laws' for large language models."
    
*   "What are some of the 'emergent abilities' of LLMs discussed in the paper?"
    
*   "What are the main societal impacts or risks mentioned in the survey?"
    

### General / Cross-Document Questions

_(For these, set MY\_DOCUMENT\_ID = None to search all documents)_

*   "What is a 'transformer'?"
    
*   "How is 'attention' used in machine learning models?"
    
*   "Compare the architectures of BERT and the original Transformer model."
    
*   "Summarize the different model architectures discussed across all these papers."